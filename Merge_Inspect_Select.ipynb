{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e513eff2",
   "metadata": {},
   "source": [
    "# Merge, Inspect, and Select Publications \n",
    "\n",
    "*Last update: 01/22/2022*\n",
    "\n",
    "With this notebook, you can merge publication lists from different queries. It uses a data file, which is generated each time you serach for scientific publications with one of our [query notebooks](https://github.com/ai4ki/LitRev_Toolbox.git). These data files are stored in the directory `results_json`. \n",
    "\n",
    "If you follow steps 1 through 5, you end up with an Excel file that contains all your publication data without duplicates. Since there is no reliable universal identifier for a publication record, we use the papers' titles to identify duplicates. We account for possible differences in notation by pre-processing the titles. Note, however, that this method is not failsafe. You thus might want to inspect the final publication list manually to remove any remaining duplicates.        \n",
    "\n",
    "Before exporting to Excel, you can manually inspect every record in the merged publication list, and remove papers that you deem irrelevant for your analysis.\n",
    "\n",
    "## Relevance Scores\n",
    "Along the way, you can calculate a number of relevance scores, which you might want to consider in your final assessment (see step 2). These scores are added to your final Excel-file so that you can use Excel's sort and filter options to rank publications.\n",
    "\n",
    "## Working with Jupyter notebooks\n",
    "\n",
    "In case you are not familiar with Jupyter notebooks, this is how to go about it: In order to execute a piece of code, click inside a cell (the ones with `[]` to the left) and press Shift+Enter. Wait until the cell is done--that's when the `*` in `[]` turned into a number--and move on to the next cell.\n",
    "\n",
    "If you get inconceivable error messages or the notebook gets stuck, choose \"Restart & Clear Output\" in the \"Kernel\" dropdown-menu above and start afresh. \n",
    "\n",
    "___\n",
    "**Please help us to improve this tool by [emailing us](mailto:ai4ki.dev@gmail.com?subject=ai4ki-tools:%20Merge_Inspect_Select) your update ideas or error reports.**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229ce99",
   "metadata": {},
   "source": [
    "## Preparation: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb2530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ai4ki_utils.merge_rank_utils import *\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea565fbf",
   "metadata": {},
   "source": [
    "## Step 1: Fetch publications lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac03984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all JSON-files in dedicated directory\n",
    "path = './results_json'\n",
    "pub_data = get_pub_data(path=path)\n",
    "\n",
    "# Get some useful global data\n",
    "n_files = len(pub_data)\n",
    "pub_data_lengths = [len(pub_data[i]) for i in range(n_files)]\n",
    "mx_flen = max(pub_data_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ebed6",
   "metadata": {},
   "source": [
    "### 1.1: Check publication lists for bad entries\n",
    "*Run this cell to delete records without an entry in the title field as these cause trouble further down below.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_pub_data(pub_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fed2e8",
   "metadata": {},
   "source": [
    "## Step 2: Calculate relevance scores\n",
    "### 2.1: Normalized rank score\n",
    "Scientific search engines like Semantic Scholar, Google Scholar, or CORE return a ranked list of results for each query (usually called 'relevance sorting'). We use a simple technique to preserve this information in the merging process.\n",
    "\n",
    "The following cell calculates a 'Rank score' (RS) between 0 and 1 for each publication in the lists provided in step 1 (RS=0: top ranked). When merging the lists in step 3, we calculate the average RS, if a publication appears in more than one list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RS for each publication in each list and add to data\n",
    "rank_score(pub_data, mx_flen, key='Rank score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e7493",
   "metadata": {},
   "source": [
    "### 2.2: Title and abstract match score\n",
    "The following cell calculates two relevance scores: 'Title match score' (TMS) and 'Abstract match score' (AMS).\n",
    "\n",
    "Based on a list of keywords you provide, TMS is the number of keywords that appear in a publication's title. We divide this number by total number of keywords to get a TMS value between 0 and 1. AMS is defined accordingly with respect to a publication's abstract (if available). \n",
    "\n",
    "*Note: Comma-seperate your list of keywords:* `deep learning, cats, dogs, classification algorithm`.<br>Make sure to spell your keywords correctly!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TMS and AMS for each publication in each list and add to data\n",
    "keywords = input('Enter keywords: ')\n",
    "keywords = keywords.split(',')\n",
    "keywords = [k.strip().lower() for k in keywords]\n",
    "match_score(pub_data, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610c6a8",
   "metadata": {},
   "source": [
    "### 2.3.: Similarity Score \n",
    "The following cell calculates a similiarity score (SimS) between a user-provided document and the abstract of a publication. SimS is based on the [TF-IDF-algorithm](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Term Frequencyâ€“Inverse Document Frequency). SimS ranges from 0 to 1, with a value of 1 indicating identical documents.\n",
    "\n",
    "Provide your document as a `.txt`-file and upload it to the JupyterLab base directory. Run the following cell and enter the full filename: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and read base document\n",
    "filename = input('Enter filename: ')\n",
    "with open(filename, 'r', encoding='utf-8') as f: document = f.read()\n",
    "document = document.replace('\\n',' ')\n",
    "\n",
    "# Calculate similarity score for each publication in each list and add to data\n",
    "sim_score(pub_data, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b684d0",
   "metadata": {},
   "source": [
    "## Step 3: Merge publication data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8403d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get the titles from the data and process them for matching\n",
    "pub_data_titles, pub_data_idx = get_proc_titles(pub_data)\n",
    "\n",
    "# Second, deduplicate and merge publication data\n",
    "pub_data_merge_final = merge_pub_data(pub_data, pub_data_titles, pub_data_idx)\n",
    "df_merge = pd.DataFrame(pub_data_merge_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996b220",
   "metadata": {},
   "source": [
    "### Step 3.1: Display merged publication data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview merged results\n",
    "n_rows = int(input('Enter number of rows to display: ')) \n",
    "c_sort = input('Enter name of column for sorting: ')\n",
    "\n",
    "if c_sort != '0': \n",
    "    try:\n",
    "        df_merge.sort_values(by=c_sort, inplace=True, ascending=True)\n",
    "        print('==> Displaying first {} items, sorted by column {}:'.format(n_rows, c_sort))\n",
    "    except:\n",
    "        print('ERROR: Selceted sorting column does not exist--check spelling!')\n",
    "        print('==> Showing unsorted publication list instead:')\n",
    "df_merge.head(n_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a989e2f",
   "metadata": {},
   "source": [
    "## Step 4: Manually inspect merged publication data\n",
    "*Walk through the merged publication records and decide which ones to keep.*<br>\n",
    "*It's best to do this in one pass, but in case you get tired you can exit by entering 'stop'...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_index, pub_data_final = inspect_pubs(pub_data_merge_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbcc71c",
   "metadata": {},
   "source": [
    "### Step 4.1: Continue manual inspection of merged publication data (just in case...)\n",
    "*In case you interrupted manual inspection of publication data above, run the following cell!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfa577",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ir_index != 0:\n",
    "    ir_index, pub_data_cntd = inspect_pubs(pub_data_merge_final, idx_start=ir_index)\n",
    "    pub_data_final.append(pub_data_cntd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408afcf",
   "metadata": {},
   "source": [
    "## Step 5: Export final publication list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959809b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "XCL_XTNSN, JSN_XTNSN, BBT_XTNSN = '.xlsx', '.json', '.bib'\n",
    "outfile = 'Final_Publication_List'\n",
    "\n",
    "# Choose which data to export\n",
    "print('Which publication data do you want to export?')\n",
    "which_data = input('auto/manual ')\n",
    "if which_data == 'auto':\n",
    "    data_out = pub_data_merge_final\n",
    "elif which_data == 'manual':\n",
    "    data_out = pub_data_final    \n",
    "\n",
    "# Export results to EXCEL file\n",
    "df_out = pd.DataFrame(data_out)\n",
    "df_out.to_excel(join('./results_merge',outfile+XCL_XTNSN), engine='xlsxwriter', index=False)\n",
    "\n",
    "# Export results to JSON file\n",
    "with open(join('./results_merge',outfile+JSN_XTNSN), 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_out, f)\n",
    "    \n",
    "# Export BibTex-Data to .bib-file\n",
    "bibtex_data = '\\n\\n'.join([item['BibTex'] for item in data_out if item['BibTex'] is not None])\n",
    "with open(join('./results_merge',outfile+BBT_XTNSN), 'w', encoding='utf-8') as f:\n",
    "    f.write(bibtex_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d8c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai4ki]",
   "language": "python",
   "name": "conda-env-ai4ki-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
